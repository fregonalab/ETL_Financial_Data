{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual Financial Statements - SimFin API\n",
    "\n",
    "**Luiz Guilherme Gomes Fregona**\n",
    "\n",
    "12/10/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Objetive](#objetive)\n",
    "3. [Pipeline](#pipeline)\n",
    "4. [Imports](#imports)\n",
    "5. [Extract Data from API](#extraction)\n",
    "    1. [Call the API](#api)\n",
    "    2. [Save as Dataframe](#save)\n",
    "7. [Extract, Transform, and Load (ETL)](#etl)\n",
    "    1. [Set Paths](#paths)\n",
    "    2. [Extract](#extract)\n",
    "    3. [Tranform](#transform)\n",
    "    4. [Load](#load)\n",
    "    5. [Logging](#logging)\n",
    "    6. [Running ETL process](#running)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of data for prediction of stocks has grown in the last 5 years. Both robots and analysts have started to make predictions, and estimates based on financial data, which are regulated and can be found in SEC EDGAR website. In this project, we focused on acquiring financial data from an open-source API called SimFin. The main documents extracted from the API are the income statement, balance sheet, and cash flow of Apple and Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetives <a name=\"objetive\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goal of this project is to develop an ETL process capable of unifying all three documents into a single .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline <a name=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline decided for this project includes 4 major steps:\n",
    "\n",
    "1. Extract financial documents from API, and convert each one separetely .csv files\n",
    "2. Extract .csv files from local repo and store them into a pandas dataframe\n",
    "3. Tranform data following certain criterias\n",
    "4. Load the unified dataframe into a final .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries will be required before moving forward with this project. The following code check if you already have those libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the packages are installed in the desktop\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the requests package! Run !pip install requests and try again.\")\n",
    "    \n",
    "try:\n",
    "    import pandas\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the pandas package! Run !pip install pandas and try again.\")\n",
    "\n",
    "try:\n",
    "    import os\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the os package! Run !pip install os and try again.\")\n",
    "    \n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the os package! Run !pip install numpy and try again.\")\n",
    "\n",
    "try:\n",
    "    from datetime import datetime\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the os package! Run !pip install datetime and try again.\")\n",
    "\n",
    "try:\n",
    "    import glob\n",
    "except ImportError:\n",
    "    sys.exit(\"You need the os package! Run !pip install glob and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, you don't have them. Please remove the \"#\" symbol from the code cells below before running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install pandas\n",
    "#!pip install os\n",
    "#!pip install numpy\n",
    "#!pip install datetime\n",
    "#!pip install glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from API <a name=\"extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimFin is a open-source API which provides financial statements and fundamentals about the main public companies in US. SimFin provides two types of APIs: Python API and WEB API. Here, we are going to use the WEB API. The API key was obtained at https://simfin.com/data/api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"f6Dh7PNT3yDf1DRct0XJ2tdeZhY2U8SQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, the goal is to get the income statement, the balance sheet, and the cash flow from apple & google since 2010 until now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the stock ticker\n",
    "tickers = [\"AAPL\",\"GOOG\"]\n",
    "\n",
    "#Define the periods and years\n",
    "period = \"fy\"\n",
    "year_start = 2010\n",
    "year_end = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endpoint for all financial statements\n",
    "request_url = 'https://simfin.com/api/v2/companies/statements'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API <a name=\"api\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calling the API, we are going to set all scientific notation to 5 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve matching filings from the API, we are going to send a HTTP POST request to the API with a dictionary formatted payload for each combinating of years, periods, and tickers. The query format follows the recomendations presented at https://simfin.com/api/v2/documentation/#tag/Company/paths/~1companies~1list/get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income Statements\n",
    "\n",
    "# variable to store the names of the columns\n",
    "income_columns = []\n",
    "# variable to store our data\n",
    "income_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    #loop through all years\n",
    "    for year in range(year_start, year_end + 1):\n",
    "\n",
    "                # define the parameters for the query\n",
    "                parameters = {\"statement\": \"pl\", \"ticker\": ticker, \"period\": period, \"fyear\": year, \"api-key\": api_key}\n",
    "                # make the request\n",
    "                request = requests.get(request_url, parameters)\n",
    "\n",
    "                # convert response to json and take 0th index\n",
    "                data = request.json()[0]\n",
    "\n",
    "                # make sure that data was found\n",
    "                if data['found'] and len(data['data']) > 0:\n",
    "                    # add the column descriptions once only\n",
    "                    if len(income_columns) == 0:\n",
    "                        income_columns = data['columns']\n",
    "                    # add the data\n",
    "                    income_data += data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balance Sheet\n",
    "\n",
    "# variable to store the names of the columns\n",
    "balance_columns = []\n",
    "# variable to store our data\n",
    "balance_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    #loop through all years\n",
    "    for year in range(year_start, year_end + 1):\n",
    "\n",
    "                # define the parameters for the query\n",
    "                parameters = {\"statement\": \"bs\", \"ticker\": ticker, \"period\": period, \"fyear\": year, \"api-key\": api_key}\n",
    "                # make the request\n",
    "                request = requests.get(request_url, parameters)\n",
    "\n",
    "                # convert response to json and take 0th index\n",
    "                data = request.json()[0]\n",
    "\n",
    "                # make sure that data was found\n",
    "                if data['found'] and len(data['data']) > 0:\n",
    "                    # add the column descriptions once only\n",
    "                    if len(balance_columns) == 0:\n",
    "                        balance_columns = data['columns']\n",
    "                    # add the data\n",
    "                    balance_data += data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cash Flow\n",
    "\n",
    "# variable to store the names of the columns\n",
    "flow_columns = []\n",
    "# variable to store our data\n",
    "flow_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    #loop through all years\n",
    "    for year in range(year_start, year_end + 1):\n",
    "\n",
    "                # define the parameters for the query\n",
    "                parameters = {\"statement\": \"cf\", \"ticker\": ticker, \"period\": period, \"fyear\": year, \"api-key\": api_key}\n",
    "                # make the request\n",
    "                request = requests.get(request_url, parameters)\n",
    "\n",
    "                # convert response to json and take 0th index\n",
    "                data = request.json()[0]\n",
    "\n",
    "                # make sure that data was found\n",
    "                if data['found'] and len(data['data']) > 0:\n",
    "                    # add the column descriptions once only\n",
    "                    if len(flow_columns) == 0:\n",
    "                        flow_columns = data['columns']\n",
    "                    # add the data\n",
    "                    flow_data += data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as Dataframe <a name=\"save\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep data organized, the data will be stored in .csv files separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build dataframes\n",
    "df_income = pd.DataFrame(income_data, columns = income_columns)\n",
    "df_balance = pd.DataFrame(balance_data, columns = balance_columns)\n",
    "df_cash_flow = pd.DataFrame(flow_data, columns = flow_columns)\n",
    "\n",
    "#Check what is the current working directory\n",
    "os.getcwd()\n",
    "\n",
    "#Save as .csv files\n",
    "df_income.to_csv(\"income.csv\", index = False)\n",
    "df_balance.to_csv(\"balance.csv\", index = False) \n",
    "df_cash_flow.to_csv(\"cash_flow.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform, and Load (ETL) <a name=\"etl\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the whole process of Extract, Tranform, and Load organized, a set of documents and functions will be created first. The documents are a .csv file, where all the information will be stored and read to be uploaded to a relational database, and a .txt logging file to keep track of all steps throughout the ETL process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Paths <a name=\"paths\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile    = \"logfile.txt\"      # all event logs will be stored in this file\n",
    "targetfile = \"final_data.csv\"   # file where transformed data is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract <a name=\"extract\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extract function converts all .csv files into one single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV extract function\n",
    "def extract_from_csv(file_to_process):\n",
    "    dataframe = pd.read_csv(file_to_process)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files extract function\n",
    "def extract():\n",
    "    \n",
    "    # variable to store the names of the columns\n",
    "    columns = []\n",
    "    # variable to store our data\n",
    "    data = []\n",
    "    \n",
    "    #process all csv files\n",
    "    for csvfile in glob.glob(\"*.csv\"):\n",
    "            raw_data = extract_from_csv(csvfile)\n",
    "            extracted_columns = raw_data.columns\n",
    "            extracted_list = raw_data.transpose().to_numpy().tolist()\n",
    "            columns.extend(extracted_columns)\n",
    "            data.extend(extracted_list)\n",
    "\n",
    "    #Transpose data list\n",
    "    data = np.array(data).T.tolist()\n",
    "\n",
    "    #Build final dataframe\n",
    "    extracted_data = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform <a name=\"transform\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform function does the following tasks:\n",
    "\n",
    "1. Fill \"nan\" with 0\n",
    "2. Get rid of all unnecessary columns\n",
    "3. Define all datatypes correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "        #Fill \"nan\" values with 0\n",
    "        data = data.replace(\"nan\", 0)\n",
    "        \n",
    "        #Filter duplicated columns\n",
    "        data = data.loc[:,~data.columns.duplicated()]\n",
    "        \n",
    "        #Get rid of useless columns\n",
    "        useless_col = [\"SimFinId\",\"Fiscal Period\", \"Restated Date\", \"Publish Date\", \"Source\", \"TTM\", \"Value Check\"]\n",
    "        data = data[data.columns[~data.columns.isin(useless_col)]]\n",
    "        \n",
    "        #Convert categorical objects into strings, and date into date\n",
    "        data['Report Date'] =  pd.to_datetime(data['Report Date'], utc = False)\n",
    "        data[['Ticker','Fiscal Year']] = data[['Ticker','Fiscal Year']].astype(str)\n",
    "        \n",
    "        #The remaining objects were turn into numeric features\n",
    "        col = data.columns.drop(['Ticker','Fiscal Year','Report Date'])\n",
    "        data[col] = data[col].apply(pd.to_numeric)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load function add the final dataframe to a .csv file ready to be uploaded into a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(targetfile,data_to_load):\n",
    "    data_to_load.to_csv(targetfile)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging <a name=\"logging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logging function help us keep track of all updates performed during the ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' # Year-Monthname-Day-Hour-Minute-Second\n",
    "    now = datetime.now() # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(\"logfile.txt\",\"a\") as f:\n",
    "        f.write(timestamp + ',' + message + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ETL process  <a name=\"running\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"ETL Job Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Extract phase Started\")\n",
    "extracted_data = extract()\n",
    "log(\"Extract phase Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Transform phase Started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "log(\"Transform phase Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Load phase Started\")\n",
    "load(targetfile,transformed_data)\n",
    "log(\"Load phase Ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"ETL Job Ended\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
